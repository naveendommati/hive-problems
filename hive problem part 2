
1 ) Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?
answer:---
CREATE EXTERNAL TABLE sample(id int, first_name string, last_name string, email string, gender string, ip_address string) 
ROW FORMAT SERDE ‘org.apache.hadoop.hive.serde2.OpenCSVSerde’ 
STORED AS TEXTFILE LOCATION ‘/temp’;

2) Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?
answer:-- 
### first we  will create temporary table in hive and load data into temporary table
CREATE TABLE temporary(id INT, name STRING, e-mail STRING, country STRING)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ‘,’ 
STORED AS TEXTFILE;
load data local inpath 'file///confige/bigdata/ good'INTO TABLE temporary;

## now we will create a permanant table where all files need to move.
CREATE TABLE sample(id INT, name STRING, e-mail STRING, country STRING)
ROW FORMAT DELIMITED  
FIELDS TERMINATED BY ‘,’
STORED AS SEQUENCEFILE;
insert Overwrite table sample select * from temporary;

##all data will be transfered from old table to permenant table with out losing of data.


3) Will the reducer work or not if you use “Limit 1” in any HiveQL query?
answer:--  I think Reducer will work, because as per Hive documentation -- Limit indicates the number of rows to be returned.
           The rows returned are chosen at random.
           
 4) Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration.
 Then, what will happen if we have multiple clients trying to access Hive at the same time? 
 answer:-- The default metastore configuration allows only one Hive session to be opened at a time for accessing the metastore.
 Therefore, if multiple clients try to access the metastore at the same time, they will get an error. 
 One has to use a standalone metastore, i.e. Local or remote metastore configuration in Apache Hive for allowing access to multiple clients concurrently. 

5)Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?
answer:-- We can solve this problem of query latency by partitioning the table according to each month. 
So, for each month we will be scanning only the partitioned data instead of whole data sets.

CREATE TABLE partitioned_transaction (cust_id INT, amount FLOAT, country STRING)
PARTITIONED BY (month STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ‘,’ ;

SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;

INSERT OVERWRITE TABLE partitioned_transaction PARTITION (month) SELECT cust_id, amount, country, month FROM transaction_details;


6) How can you add a new partition for the month December in the above partitioned table?
answer:-- ALTER TABLE partitioned_transaction ADD PARTITION (month=’Dec’) LOCATION  ‘/partitioned_transaction’;


7) I am inserting data into a table based on partitions dynamically.
But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. 
How will you remove this error?
answer:-- To remove this error one has to execute following commands: 
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;


8) LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;
The following statement failed to execute. What can be the cause?
answer:-- The local inpath should contain a file path and not a directory. The HOME is a valid variable available in the hive environment.

9) Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?
answer:-- Yes, we can add the nodes by following the below steps:

Step 1: Take a new system; create a new username and password
Step 2: Install SSH and with the master node setup SSH connections
Step 3: Add ssh public_rsa id key to the authorized keys file
Step 4: Add the new DataNode hostname, IP address, and other details in /etc/hosts slaves file:192.168.1.102 slave3.in slave3
Step 5: Start the DataNode on a new node
Step 6: Login to the new node like suhadoop or: ssh -X hadoop@192.168.1.103
Step 7: Start HDFS of the newly added slave node by using the following command:./bin/hadoop-daemon.sh start data node
Step 8: Check the output of the jps command on the new node

10) Hive Join operations.
Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
)?
answer: --
create table customer(id int, name string, age int, address string, salary int)
row format delimited
fields terminated by ','
stored as textfile;
create table order(id int, date date, customer_id int, amount int)
row format delimited
fields terminated by ',';

select * from customer c INNER JOIN order o on c.id =o.id:
select * from customer c left join order o on c.id= customer_id ;
select * from customer c right join order o on c.id = customer_id ;
select * from customer c full outer join order o ;

11) Download a data from the given location - 
https://archive.ics.uci.edu/ml/machine-learning-databases/00360/

1. Create a hive table as per given schema in your dataset 
create table airquality1(name string , code string, CO string, PT08_S1 int, NMHC smallint, C6H6 string, PT08_S2  int, NOx  smallint, PT08_S3  int, NO2  smallint, PT08_S4 int, PT08_S5 int, T string, RH string, AH string )
row format delimited
fields terminated by ';'
tblproperties("skip.header.line.count" = "1");

2. try to place a data into table location
load data local inpath 'file:///config/workspace/quality' into table airquality1;

3.Perform a select operation 
select * from airquality1 limit 3;

4. Fetch the result of the select operation in your local as a csv file .
same records available with out null values.

5. Perform group by operation 
select name, sum(T) from airquality1 group by name;

9. alter table operation 
alter table airquality1 change T temperature string;

10 . drop table operation
drop table airquality1;


